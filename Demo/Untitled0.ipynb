{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen_vl_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXfRTOR5t-SI",
        "outputId": "d07a53d6-8636-40a6-f64d-f415a2d2a578"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qwen_vl_utils\n",
            "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting av (from qwen_vl_utils)\n",
            "  Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from qwen_vl_utils) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen_vl_utils) (11.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from qwen_vl_utils) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (2024.12.14)\n",
            "Downloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
            "Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av, qwen_vl_utils\n",
            "Successfully installed av-14.0.1 qwen_vl_utils-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcM0ycYWtyqy",
        "outputId": "ecec5e24-294b-478b-f542-20c0b581e545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"ROYAUME DU MAROC\\nCARTE NATIONALE D'IDENTITE\\nMALAK\\nHAFFANE\\nNée le\\n23.09.2006\\nà MAARIF CASABLANCA ANFA\\nValable jusqu'au\\n26.12.2028\\nFG\\nBJ472690<|im_end|>\"]\n",
            "Final memory allocated: 5150672896\n",
            "Final memory reserved: 9770631168\n"
          ]
        }
      ],
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "# Set environment variable for expandable segments\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Load the model with reduced precision and enable flash attention\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",  # Let accelerate handle the device mapping\n",
        ")\n",
        "\n",
        "# Default processor\n",
        "processor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\")\n",
        "\n",
        "# Prepare messages for inference\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": \"/content/card.png\",\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"Extract the text from the ID image\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "# Preparation for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "# Clear any cached memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Move inputs to GPU\n",
        "inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}\n",
        "\n",
        "# Inference: Generation of the output\n",
        "try:\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "\n",
        "    # Trim the generated IDs to get the output text\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)\n",
        "    ]\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    # Print the output text\n",
        "    print(output_text)\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if 'out of memory' in str(e):\n",
        "        print(\"CUDA out of memory. Try reducing the batch size or using mixed precision.\")\n",
        "        torch.cuda.empty_cache()  # Clear cache to free up memory\n",
        "    else:\n",
        "        raise e\n",
        "\n",
        "print(\"Final memory allocated:\", torch.cuda.memory_allocated())\n",
        "print(\"Final memory reserved:\", torch.cuda.memory_reserved())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4PriEPpt9ue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}